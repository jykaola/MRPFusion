import torch
import torch.nn as nn
import torch.nn.functional as F


class SeqConv3x3(nn.Module):
    #seq_typeæŒ‡å®šåºåˆ—å·ç§¯çš„ç±»å‹
    '''
    SeqConv3x3 ç±»æ”¯æŒå››ç§ç±»å‹çš„åºåˆ—å·ç§¯ï¼š
    'conv1x1-conv3x3': å…ˆæ‰§è¡Œä¸€ä¸ª1x1å·ç§¯ï¼Œç„¶åæ˜¯ä¸€ä¸ª3x3å·ç§¯ã€‚
    'conv1x1-sobelx': å…ˆæ‰§è¡Œä¸€ä¸ª1x1å·ç§¯ï¼Œç„¶ååº”ç”¨Sobelæ°´å¹³è¾¹ç¼˜æ£€æµ‹ç®—å­ã€‚
    'conv1x1-sobely': å…ˆæ‰§è¡Œä¸€ä¸ª1x1å·ç§¯ï¼Œç„¶ååº”ç”¨Sobelå‚ç›´è¾¹ç¼˜æ£€æµ‹ç®—å­ã€‚
    'conv1x1-laplacian': å…ˆæ‰§è¡Œä¸€ä¸ª1x1å·ç§¯ï¼Œç„¶ååº”ç”¨Laplacianè¾¹ç¼˜æ£€æµ‹ç®—å­ã€‚

    inp_planes è¡¨ç¤ºè¾“å…¥ç‰¹å¾å›¾çš„é€šé“æ•°
    out_planes è¾“å‡ºç‰¹å¾å›¾çš„é€šé“æ•°
    depth_multiplier è°ƒæ•´ç¬¬ä¸€ä¸ª1x1å·ç§¯å±‚çš„è¾“å‡ºé€šé“æ•°ï¼Œ1x1å·ç§¯é€šå¸¸ç”¨äºé™ç»´æˆ–å‡ç»´ï¼Œå³å‡å°‘æˆ–å¢åŠ ç‰¹å¾å›¾çš„é€šé“æ•°
    depth_multiplierå¯ä»¥å¤§äº1ã€ç­‰äº1æˆ–å°äº1ï¼Œåˆ†åˆ«ç”¨äºå¢åŠ ã€ä¿æŒä¸å˜æˆ–å‡å°‘è¾“å‡ºé€šé“æ•°
    '''
    def __init__(self, seq_type, inp_planes, out_planes, depth_multiplier):
        super(SeqConv3x3, self).__init__()

        self.type = seq_type
        self.inp_planes = inp_planes
        self.out_planes = out_planes

        if self.type == 'conv1x1-conv3x3':
            #å®šä½äºä¸¤ä¸ªå·ç§¯æ“ä½œï¼ˆ1x1å·ç§¯å’Œ3x3å·ç§¯ï¼‰ä¹‹é—´çš„ç‰¹å¾å›¾çš„é€šé“æ•°
            self.mid_planes = int(out_planes * depth_multiplier)
            conv0 = torch.nn.Conv2d(self.inp_planes, self.mid_planes, kernel_size=1, padding=0)
            self.k0 = conv0.weight
            self.b0 = conv0.bias

            conv1 = torch.nn.Conv2d(self.mid_planes, self.out_planes, kernel_size=3)
            self.k1 = conv1.weight
            self.b1 = conv1.bias

        #ç”¨äºåº”ç”¨Sobelè¾¹ç¼˜æ£€æµ‹ç®—å­ä¸­çš„æ°´å¹³æ–¹å‘è¾¹ç¼˜æ£€æµ‹
        elif self.type == 'conv1x1-sobelx':
            conv0 = torch.nn.Conv2d(self.inp_planes, self.out_planes, kernel_size=1, padding=0)
            self.k0 = conv0.weight
            self.b0 = conv0.bias


            '''
            Sobelç®—å­ä¸æ”¹å˜é€šé“æ•°ï¼Œæ‰€ä»¥åœ¨Sobelä¹‹å‰çš„1*1å·ç§¯æ“ä½œï¼Œç›´æ¥ä»inp_planesåˆ°out_planes
            åˆå§‹åŒ–Sobelç®—å­çš„å‚æ•°ï¼ŒåŒ…æ‹¬å°ºåº¦ï¼ˆscaleï¼‰ã€åç½®ï¼ˆbiasï¼‰å’Œæ©ç ï¼ˆmaskï¼‰
            scaleå’Œbiaséƒ½æ˜¯å¯å­¦ä¹ çš„å‚æ•°ï¼Œå®ƒä»¬ç”¨äºç¼©æ”¾å’Œåç§»Sobelç®—å­çš„ç»“æœ
            self.out_planesè¡¨ç¤ºæ•°é“æ•°ç›¸åŒ¹é…ï¼Œåä¸‰ç»´åº¦1 x 1 x 1ä¼šé€šè¿‡å¼ é‡â€œå¹¿æ’­â€åˆ°ä¸è¾“å‡ºç‰¹å¾å›¾ç›¸åŒçš„å½¢çŠ¶
            å…¶ä¸­çš„å…ƒç´ æ˜¯ä»å‡å€¼ä¸º0ã€æ ‡å‡†å·®ä¸º1çš„æ­£æ€åˆ†å¸ƒä¸­éšæœºé‡‡æ ·çš„ã€‚è¿™æ„å‘³ç€æ¯ä¸ªé€šé“çš„scaleå€¼éƒ½æ˜¯éšæœºçš„ï¼Œè€Œä¸”å¯èƒ½æ­£ä¹Ÿå¯èƒ½è´Ÿã€‚
            éšåçš„ * 1e-3æ“ä½œåˆ™æ˜¯å°†è¿™äº›éšæœºç”Ÿæˆçš„å€¼å…¨éƒ¨ä¹˜ä»¥1e - 3ï¼ˆå³0.001ï¼‰
            '''

            scale = torch.randn(size=(self.out_planes, 1, 1, 1)) * 1e-3
            #ç”Ÿæˆä¸€ä¸ªéšæœºåˆ†å¸ƒçš„å¼ é‡å°†å…¶ä¹˜ä»¥ä¸€ä¸ªå°çš„å¸¸æ•° 1e-3
            #scaleè¢«åŒ…è£…æˆnn.Parameteråï¼Œè¢«æ·»åŠ åˆ°æ¨¡å—ï¼ˆmodelï¼‰çš„å‚æ•°åˆ—è¡¨ä¸­ï¼Œåå‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œscaleçš„å€¼å°±ä¼šæ ¹æ®æŸå¤±å‡½æ•°çš„æ¢¯åº¦è‡ªåŠ¨æ›´æ–°
            self.scale = nn.Parameter(scale)
            # bias = 0.0
            # bias = [bias for c in range(self.out_planes)]
            # bias = torch.FloatTensor(bias)
            bias = torch.randn(self.out_planes) * 1e-3
            #é‡å¡‘ä¸ºä¸€ä¸ªå½¢çŠ¶ä¸º(self.out_planes,)çš„ä¸€ç»´å¼ é‡
            bias = torch.reshape(bias, (self.out_planes,))
            #torch.reshape()ï¼šè¿™æ˜¯ PyTorch ä¸­ç”¨äºæ”¹å˜å¼ é‡å½¢çŠ¶çš„æ–¹æ³•ã€‚å®ƒæ¥å—ä¸¤ä¸ªå‚æ•°ï¼šç¬¬ä¸€ä¸ªå‚æ•°æ˜¯è¦è¢«é‡å¡‘çš„å¼ é‡ã€‚
            # ç¬¬äºŒä¸ªå‚æ•°æ˜¯ä¸€ä¸ªå…ƒç»„ï¼ŒæŒ‡å®šäº†æ–°çš„å½¢çŠ¶ã€‚è¿™é‡Œçš„å½¢çŠ¶æ˜¯ä¸€ä¸ªåªåŒ…å«ä¸€ä¸ªå…ƒç´ çš„å…ƒç»„ï¼Œå³ (self.out_planes,)ã€‚
            self.bias = nn.Parameter(bias)
            # åˆ›å»ºäº†ä¸€ä¸ªå½¢çŠ¶ä¸º(self.out_planes, 1, 3, 3)çš„å¼ é‡, self.out_planes æ˜¯è¾“å‡ºç‰¹å¾å›¾çš„é€šé“æ•°,
            # 1 è¡¨ç¤ºè¾“å…¥ç‰¹å¾å›¾çš„é€šé“è¾“ã€‚éœ€è¦æ³¨æ„ï¼šmaskå¹¶ä¸æ˜¯ä¸€ä¸ªå·ç§¯æ ¸ï¼Œå¯ä»¥æŠŠmaskçœ‹ä½œout_planesæ·±åº¦ä¸º1çš„å·ç§¯æ ¸ï¼Œä¹Ÿå°±æ˜¯è¯´
            # ä¸€ä¸ªæ·±åº¦ä¸º1çš„å·ç§¯æ ¸ä½œç”¨äºä¸€ä¸ªè¾“å…¥ç‰¹å¾å›¾çš„é€šé“
            self.mask = torch.zeros((self.out_planes, 1, 3, 3), dtype=torch.float32)
            for i in range(self.out_planes):
                self.mask[i, 0, 0, 0] = 1.0
                self.mask[i, 0, 1, 0] = 2.0
                self.mask[i, 0, 2, 0] = 1.0
                self.mask[i, 0, 0, 2] = -1.0
                self.mask[i, 0, 1, 2] = -2.0
                self.mask[i, 0, 2, 2] = -1.0
            self.mask = nn.Parameter(data=self.mask, requires_grad=False)#Sobelç®—å­ä¸­ï¼Œæ©ç çš„å€¼æ˜¯å›ºå®šçš„ï¼Œç”¨äºæ£€æµ‹è¾¹ç¼˜ï¼Œä¸éœ€è¦é€šè¿‡è®­ç»ƒæ¥è°ƒæ•´

        elif self.type == 'conv1x1-sobely':
            conv0 = torch.nn.Conv2d(self.inp_planes, self.out_planes, kernel_size=1, padding=0)
            self.k0 = conv0.weight
            self.b0 = conv0.bias

            # init scale & bias
            scale = torch.randn(size=(self.out_planes, 1, 1, 1)) * 1e-3
            self.scale = nn.Parameter(torch.FloatTensor(scale))
            # bias = 0.0
            # bias = [bias for c in range(self.out_planes)]
            # bias = torch.FloatTensor(bias)
            bias = torch.randn(self.out_planes) * 1e-3
            bias = torch.reshape(bias, (self.out_planes,))
            self.bias = nn.Parameter(torch.FloatTensor(bias))
            #nn.Parameter(torch.FloatTensor(bias))è½¬æ¢ä¸ºä¸€ä¸ª torch.FloatTensor ç±»å‹çš„å¼ é‡ï¼Œç„¶åå°†å…¶åŒ…è£…ä¸º nn.Parameter ç±»å‹
            # init mask
            self.mask = torch.zeros((self.out_planes, 1, 3, 3), dtype=torch.float32)
            for i in range(self.out_planes):
                self.mask[i, 0, 0, 0] = 1.0
                self.mask[i, 0, 0, 1] = 2.0
                self.mask[i, 0, 0, 2] = 1.0
                self.mask[i, 0, 2, 0] = -1.0
                self.mask[i, 0, 2, 1] = -2.0
                self.mask[i, 0, 2, 2] = -1.0
            self.mask = nn.Parameter(data=self.mask, requires_grad=False)

        elif self.type == 'conv1x1-laplacian':
            conv0 = torch.nn.Conv2d(self.inp_planes, self.out_planes, kernel_size=1, padding=0)
            self.k0 = conv0.weight
            self.b0 = conv0.bias

            # init scale & bias
            scale = torch.randn(size=(self.out_planes, 1, 1, 1)) * 1e-3
            self.scale = nn.Parameter(torch.FloatTensor(scale))
            # bias = 0.0
            # bias = [bias for c in range(self.out_planes)]
            # bias = torch.FloatTensor(bias)
            bias = torch.randn(self.out_planes) * 1e-3
            bias = torch.reshape(bias, (self.out_planes,))
            self.bias = nn.Parameter(torch.FloatTensor(bias))
            # æ‹‰æ™®æ‹‰æ–¯ç®—å­æ©ç 
            self.mask = torch.zeros((self.out_planes, 1, 3, 3), dtype=torch.float32)
            for i in range(self.out_planes):
                self.mask[i, 0, 0, 1] = 1.0
                self.mask[i, 0, 1, 0] = 1.0
                self.mask[i, 0, 1, 2] = 1.0
                self.mask[i, 0, 2, 1] = 1.0
                self.mask[i, 0, 1, 1] = -4.0
            self.mask = nn.Parameter(data=self.mask, requires_grad=False)
        else:
            raise ValueError('the type of seqconv is not supported!')

    def forward(self, x):
        if self.type == 'conv1x1-conv3x3':
            # conv-1x1
            y0 = F.conv2d(input=x, weight=self.k0, bias=self.b0, stride=1)
            # explicitly padding with biaså¯¹y0è¿›è¡Œå¡«å…… å‘¨å›´å¡«å……1ä¸ªåƒç´ çš„ç©ºç™½
            #ä¹‹æ‰€ä»¥è¦æœ‰è¿™ä¸ªpaddingæ“ä½œ åº”è¯¥æ˜¯ä¸ºä¿è¯åé¢3*3çš„å·ç§¯çš„è¾“å‡ºå¤§å°ä¸€è‡´
            y0 = F.pad(y0, (1, 1, 1, 1), 'constant', 0)
            #ä¸ºäº†å°†self.b0å¼ é‡çš„å½¢çŠ¶ä»ä¸€ç»´è½¬æ¢ä¸ºå››ç»´ï¼Œä»¥é€‚åº”åç»­çš„å¹¿æ’­æ“ä½œ å½¢çŠ¶å˜ä¸º (1, self.out_planes, 1, 1)
            #ç¬¬ä¸€ä¸ª 1 ä»£è¡¨æ‰¹é‡ç»´åº¦ å› ä¸ºåç½®æ˜¯ä½œç”¨äºè¾“å‡ºçš„ç‰¹å¾å›¾çš„ï¼Œè€Œæ¯ä¸ªç‰¹å¾å›¾çš„ç»´åº¦æ˜¯(Batch size,channels,H,W)
            #-1 ä¼šè‡ªé€‚åº”ä¸ºchannelsï¼Œåœ¨è¿™é‡Œå°±æ˜¯out_planesï¼Œå…¶ä¸­æ‰¹æ¬¡1ä¹Ÿä¼šå› ä¸ºå¹¿æ’­æœºåˆ¶è¿›è¡Œé€‚åº”
            b0_pad = self.b0.view(1, -1, 1, 1)
            #å¯¹y0çš„è¾¹ç•ŒåŠ ä¸Šåç½®
            y0[:, :, 0:1, :] = b0_pad
            y0[:, :, -1:, :] = b0_pad
            y0[:, :, :, 0:1] = b0_pad
            y0[:, :, :, -1:] = b0_pad
            # conv-3x3
            y1 = F.conv2d(input=y0, weight=self.k1, bias=self.b1, stride=1)
        else:
            y0 = F.conv2d(input=x, weight=self.k0, bias=self.b0, stride=1)
            # explicitly padding with bias
            y0 = F.pad(y0, (1, 1, 1, 1), 'constant', 0)
            b0_pad = self.b0.view(1, -1, 1, 1)
            y0[:, :, 0:1, :] = b0_pad
            y0[:, :, -1:, :] = b0_pad
            y0[:, :, :, 0:1] = b0_pad
            y0[:, :, :, -1:] = b0_pad
            # conv-3x3
            y1 = F.conv2d(input=y0, weight=self.scale * self.mask, bias=self.bias, stride=1, groups=self.out_planes)
            #æ¯ä¸ªè¾“å‡ºé€šé“åªä¼šç”¨ä¸€ä¸ªå•é€šé“çš„ 3x3 çš„å·ç§¯æ ¸å¯¹è¾“å…¥ç‰¹å¾å›¾çš„ç›¸åº”é€šé“è¿›è¡Œå·ç§¯æ“ä½œ
        return y1

    #é‡å‚æ•°åŒ–æ“ä½œ
    def rep_params(self):
        device = self.k0.get_device()
        if device < 0:#cpu
            device = None

        if self.type == 'conv1x1-conv3x3':
            # re-param conv kernel
            # self.k0 = conv0.weight  (self.mid_planesï¼Œself.inp_planesï¼Œ1ï¼Œ1)
            # self.k1 = conv1.weight  (self.out_planesï¼Œself.mid_planesï¼Œ3ï¼Œ3) è¾“å…¥ç‰¹å¾å›¾NCHW
            #               weight =  (self.inp_planes,self.mid_planesï¼Œ1ï¼Œ1) å·ç§¯æ ¸inp_planes,mid_planes,H,W
            RK = F.conv2d(input=self.k1, weight=self.k0.permute(1, 0, 2, 3))#out_planes inp_planes 3 3
            # re-param conv bias
            RB = torch.ones(1, self.mid_planes, 3, 3, device=device) * self.b0.view(1, -1, 1, 1)
            #self.b0 æ‰©å±•ä¸ºä¸€ä¸ªå½¢çŠ¶ä¸º (1, self.mid_planes, 3, 3) çš„b0,åœ¨è¿›è¡Œk0ï¼Œb0çš„å·ç§¯çš„æ—¶å€™ï¼Œå¾—åˆ°çš„è¾“å‡ºç‰¹å¾å›¾å°±æ˜¯mid_planesé€šé“ï¼Œæ‰€ä»¥b0åç½®çš„é€šé“æ•°æ˜¯mid_planes
            RB = F.conv2d(input=RB, weight=self.k1).view(-1, ) + self.b1
            #input (1, self.mid_planes, 3, 3) NCHW
            #weight(self.out_planesï¼Œself.mid_planesï¼Œ3ï¼Œ3) out_planes,mid_planes,H,W
            #RB 1 out_planes 1 1--->out_planesä¸€ç»´
        else:
            tmp = self.scale * self.mask
            k1 = torch.zeros((self.out_planes, self.out_planes, 3, 3), device=device)
            for i in range(self.out_planes):
                k1[i, i, :, :] = tmp[i, 0, :, :]
            b1 = self.bias
            # re-param conv kernel
            RK = F.conv2d(input=k1, weight=self.k0.permute(1, 0, 2, 3))
            # re-param conv bias
            RB = torch.ones(1, self.out_planes, 3, 3, device=device) * self.b0.view(1, -1, 1, 1)
            RB = F.conv2d(input=RB, weight=k1).view(-1, ) + b1
        return RK, RB


class ECB(nn.Module):
    def __init__(self, inp_planes, out_planes, depth_multiplier, act_type='prelu', with_idt=False, deploy=False):
        #act_type æ¿€æ´»å‡½æ•°ç±»å‹
        #with_idt è¡¨ç¤ºæ˜¯å¦åœ¨ç½‘ç»œä¸­åŠ å…¥æ’ç­‰è¿æ¥ï¼ˆSkip Connectionï¼‰
        #deploy åœ¨éƒ¨ç½²æ¨¡å¼ä¸‹ï¼Œç½‘ç»œè¢«ç®€åŒ–ä¸ºä¸€ä¸ªå•ç‹¬çš„å·ç§¯å±‚ï¼Œè¿™æœ‰åŠ©äºå‡å°‘è®¡ç®—é‡å¹¶åœ¨å®é™…åº”ç”¨ä¸­åŠ é€Ÿæ¨ç†è¿‡ç¨‹ã€‚
        super(ECB, self).__init__()

        self.depth_multiplier = depth_multiplier
        self.inp_planes = inp_planes
        self.out_planes = out_planes
        self.act_type = act_type
        self.deploy = deploy

        if with_idt and (self.inp_planes == self.out_planes):
            self.with_idt = True
        else:
            self.with_idt = False
        '''
        ä¸€èˆ¬çš„æ’ç­‰è¿æ¥ï¼ˆIdentity Connectionï¼‰ï¼Œä¹Ÿç§°ä¸ºè·³è·ƒè¿æ¥ï¼ˆSkip Connectionï¼‰
        åœ¨æ’ç­‰è¿æ¥ä¸­ï¼Œè¾“å…¥ä¿¡å·å¯ä»¥ç›´æ¥ç»•è¿‡ä¸€ä¸ªæˆ–å¤šä¸ªå±‚ï¼Œå¹¶ä¸è¿™äº›å±‚çš„è¾“å‡ºç›¸åŠ ã€‚å…·ä½“æ¥è¯´ï¼Œå‡è®¾æœ‰ä¸€ä¸ªè¾“å…¥ ğ‘¥ï¼Œ
        ç»è¿‡ä¸€äº›å±‚çš„å¤„ç†åè¾“å‡ºä¸º ğ¹(ğ‘¥)ï¼Œé‚£ä¹ˆåœ¨ä½¿ç”¨æ’ç­‰è¿æ¥çš„æƒ…å†µä¸‹ï¼Œè¾“å‡ºå°†å˜ä¸º ğ¹(ğ‘¥)+ğ‘¥ã€‚
        
        '''

        if self.deploy:
            self.rbr_reparam = nn.Conv2d(self.inp_planes, self.out_planes, kernel_size=3, padding=1)
            #self.rbr_reparamæ˜¯ä¸€ä¸ª3x3çš„æ ‡å‡†å·ç§¯å±‚,åªåˆå§‹åŒ–ä¸€ä¸ª3x3çš„å·ç§¯å±‚
        else:
            #ééƒ¨ç½²æ¨¡å¼ä¸‹ï¼Œåˆå§‹åŒ–å¤šä¸ªå·ç§¯å±‚ï¼Œæ¯ä¸ªå·ç§¯å±‚æ‰§è¡Œä¸åŒçš„æ“ä½œ
            self.conv3x3 = torch.nn.Conv2d(self.inp_planes, self.out_planes, kernel_size=3, padding=1)
            self.conv1x1_3x3 = SeqConv3x3('conv1x1-conv3x3', self.inp_planes, self.out_planes, self.depth_multiplier)
            self.conv1x1_sbx = SeqConv3x3('conv1x1-sobelx', self.inp_planes, self.out_planes, -1)
            self.conv1x1_sby = SeqConv3x3('conv1x1-sobely', self.inp_planes, self.out_planes, -1)
            self.conv1x1_lpl = SeqConv3x3('conv1x1-laplacian', self.inp_planes, self.out_planes, -1)

        if self.act_type == 'prelu':
            self.act = nn.PReLU(num_parameters=self.out_planes)
        elif self.act_type == 'relu':
            self.act = nn.ReLU(inplace=True)#inplace=True ä½¿å¾— ReLU æ¿€æ´»å‡½æ•°ç›´æ¥ä¿®æ”¹è¾“å…¥å¼ é‡
        elif self.act_type == 'lrelu':
            self.act = nn.LeakyReLU()
        elif self.act_type == 'rrelu':
            self.act = nn.RReLU(lower=-0.05, upper=0.05)
        elif self.act_type == 'softplus':
            self.act = nn.Softplus()
        elif self.act_type == 'linear':
            pass
        else:
            raise ValueError('The type of activation if not support!')

    def forward(self, x):
        if self.deploy:
            return self.act(self.rbr_reparam(x))
        # \ æ˜¯è¡Œè¿æ¥ç¬¦ï¼Œç”¨äºå°†ä¸€è¡Œä»£ç æ‹†åˆ†æˆå¤šè¡Œ
        y = self.conv3x3(x) + \
            self.conv1x1_3x3(x) + \
            self.conv1x1_sbx(x) + \
            self.conv1x1_sby(x) + \
            self.conv1x1_lpl(x)
        if self.with_idt:
            y += x
        return self.act(y)

    #å°†æ¨¡å‹ä»è®­ç»ƒæ¨¡å¼è½¬æ¢ä¸ºéƒ¨ç½²æ¨¡å¼
    def switch_to_deploy(self):
        if hasattr(self, 'rbr_reparam'):#æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ç»å¤„äºéƒ¨ç½²æ¨¡å¼
            return
        #self.rep_params() è·å–åˆå¹¶åçš„å·ç§¯æ ¸å’Œåç½®
        RK, RB = self.rep_params()
        self.rbr_reparam = nn.Conv2d(self.inp_planes, self.out_planes, kernel_size=3, padding=1)
        self.rbr_reparam.weight.data = RK
        self.rbr_reparam.bias.data = RB
        #åˆ é™¤è¿™äº›å±æ€§ åˆ é™¤è¿™äº›å±æ€§çš„æ“ä½œæ˜¯ä¸ºäº†å°†æ¨¡å‹ä»è®­ç»ƒæ¨¡å¼è½¬æ¢ä¸ºéƒ¨ç½²æ¨¡å¼ã€‚åˆ é™¤åŸæœ‰çš„å¤šä¸ªå·ç§¯å±‚ï¼Œåªä¿ç•™ä¸€ä¸ªåˆå¹¶åçš„å·ç§¯å±‚ rbr_reparam
        self.__delattr__('conv3x3')
        self.__delattr__('conv1x1_3x3')
        self.__delattr__('conv1x1_sbx')
        self.__delattr__('conv1x1_sby')
        self.__delattr__('conv1x1_lpl')
        self.deploy = True

    def rep_params(self):
        K0, B0 = self.conv3x3.weight, self.conv3x3.bias
        K1, B1 = self.conv1x1_3x3.rep_params()
        K2, B2 = self.conv1x1_sbx.rep_params()
        K3, B3 = self.conv1x1_sby.rep_params()
        K4, B4 = self.conv1x1_lpl.rep_params()
        RK, RB = (K0 + K1 + K2 + K3 + K4), (B0 + B1 + B2 + B3 + B4)

        #è¿›è¡Œæ’ç­‰è¿æ¥
        if self.with_idt:
            device = RK.get_device()
            if device < 0:
                device = None
            K_idt = torch.zeros(self.out_planes, self.out_planes, 3, 3, device=device)
            for i in range(self.out_planes):
                K_idt[i, i, 1, 1] = 1.0
            B_idt = 0.0
            RK, RB = RK + K_idt, RB + B_idt
        return RK, RB


if __name__ == '__main__':
    # # test seq-conv
    x = torch.randn(1, 3, 480, 620).cuda()
    conv = SeqConv3x3('conv1x1-conv3x3', 3, 3, 2).cuda()
    y0 = conv(x)
    RK, RB = conv.rep_params()
    y1 = F.conv2d(input=x, weight=RK, bias=RB, stride=1, padding=1)
    print(torch.mean(y0 - y1))
    #è®¡ç®— y0 å’Œ y1 ä¹‹é—´çš„å‡å€¼å·®å¼‚ã€‚å¦‚æœå·®å¼‚æ¥è¿‘é›¶ï¼Œåˆ™è¯´æ˜ SeqConv3x3 çš„å®ç°å’Œé‡å‚æ•°åŒ–åçš„å·ç§¯æ“ä½œæ˜¯ä¸€è‡´çš„

    # test ecb
    x = torch.randn(1, 3, 480, 620).cuda()
    act_type = 'prelu'
    ecb = ECB(3, 64, 2, act_type=act_type, with_idt=True).cuda()
    y0 = ecb(x)

    RK, RB = ecb.rep_params()
    y1 = F.conv2d(input=x, weight=RK, bias=RB, stride=1, padding=1)
    if act_type == 'prelu':
        act = nn.PReLU(num_parameters=64).cuda()
        y1 = act(y1)
    elif act_type == 'relu':
        act = nn.ReLU(inplace=True).cuda()
        y1 = act(y1)
    elif act_type == 'rrelu':
        act = nn.RReLU(lower=-0.05, upper=0.05).cuda()
        y1 = act(y1)
    elif act_type == 'softplus':
        act = nn.Softplus().cuda()
        y1 = act(y1)
    elif act_type == 'linear':
        pass
    print(torch.mean(y0 - y1))



